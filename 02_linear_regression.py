# -*- coding: utf-8 -*-
"""02-Linear-regression.ipynb

Automatically generated by Colaboratory.


# Linear Regression

with Torch
"""

import numpy as np
import torch as tr

# Input (temp, rainfall, humidity)
inputs = np.array([[73, 67, 43], 
                   [91, 88, 64], 
                   [87, 134, 58], 
                   [102, 43, 37], 
                   [69, 96, 70]], dtype='float32')

# Targets (apples, oranges)
targets = np.array([[56, 70], 
                    [81, 101], 
                    [119, 133], 
                    [22, 37], 
                    [103, 119]], dtype='float32')

# Convert inputs and targets to tensors
inputs = tr.from_numpy(inputs)
targets = tr.from_numpy(targets)
print(inputs)
print(targets)

# Weights and biases
w = tr.randn(2, 3, requires_grad=True)
b = tr.randn(2, requires_grad=True)
print(w)
print(b)

"""**Model**

$y = X Ã— W^T + b$
"""

# @ is a matrix multiplication and .t() method is the transpose
def model(x):
    return x @ w.t() + b

# Generate predictions
preds = model(inputs)
print(preds)

# Compare with targets
print(targets)

# MSE loss
def mse(t1, t2):
    diff = t1 - t2
    return tr.sum(diff * diff) / diff.numel() # diff.numel() is the number of elements

# Compute loss
loss = mse(preds, targets)
print(loss)

# Compute gradients
loss.backward()

# Gradients for weights
print(w)
print(w.grad)

w
w.grad

with tr.no_grad():
    w -= w.grad * 1e-5
    b -= b.grad * 1e-5

# Let's verify that the loss is actually lower
loss = mse(preds, targets)
print(loss)

w.grad.zero_()
b.grad.zero_()
print(w.grad)
print(b.grad)

"""Train the model using gradient descent"""

# Generate predictions
preds = model(inputs)
print(preds)

# Calculate the loss
loss = mse(preds, targets)
print(loss)

# Compute gradients
loss.backward()
print(w.grad)
print(b.grad)

# Adjust weights & reset gradients
with tr.no_grad():
    w -= w.grad * 1e-5
    b -= b.grad * 1e-5
    w.grad.zero_()
    b.grad.zero_()

print(w)
print(b)

# Calculate loss
preds = model(inputs)
loss = mse(preds, targets)
print(loss)

"""Train for multiple epochs"""

# Train for 100 epochs
for i in range(100):
    preds = model(inputs)
    loss = mse(preds, targets)
    loss.backward()
    with tr.no_grad():
        w -= w.grad * 1e-5
        b -= b.grad * 1e-5
        w.grad.zero_()
        b.grad.zero_()

# Calculate loss
preds = model(inputs)
loss = mse(preds, targets)
print(loss)

# Predictions
preds

# Targets
targets

"""# Linear regression using PyTorch built-ins"""

import torch.nn as nn

# Input (temp, rainfall, humidity)
inputs = np.array([[73, 67, 43], 
                   [91, 88, 64], 
                   [87, 134, 58], 
                   [102, 43, 37], 
                   [69, 96, 70], 
                   [74, 66, 43], 
                   [91, 87, 65], 
                   [88, 134, 59], 
                   [101, 44, 37], 
                   [68, 96, 71], 
                   [73, 66, 44], 
                   [92, 87, 64], 
                   [87, 135, 57], 
                   [103, 43, 36], 
                   [68, 97, 70]], 
                  dtype='float32')

# Targets (apples, oranges)
targets = np.array([[56, 70], 
                    [81, 101], 
                    [119, 133], 
                    [22, 37], 
                    [103, 119],
                    [57, 69], 
                    [80, 102], 
                    [118, 132], 
                    [21, 38], 
                    [104, 118], 
                    [57, 69], 
                    [82, 100], 
                    [118, 134], 
                    [20, 38], 
                    [102, 120]], 
                   dtype='float32')

inputs = tr.from_numpy(inputs)
targets = tr.from_numpy(targets)

inputs

from torch.utils.data import TensorDataset

# Define dataset
train_ds = TensorDataset(inputs, targets)
train_ds[0:3]

from torch.utils.data import DataLoader

# Define data loader
batch_size = 5
train_dl = DataLoader(train_ds, batch_size, shuffle=True)

for xb, yb in train_dl:
    print(xb)
    print(yb)
    break

# Define model
model = nn.Linear(3, 2)
print(model.weight)
print(model.bias)

# Parameters
list(model.parameters())

# Generate predictions
preds = model(inputs)
preds

# Import nn.functional
import torch.nn.functional as F

# Define loss function
loss_fn = F.mse_loss

loss = loss_fn(model(inputs), targets)
print(loss)

"""Instead of manually manipulating the model's weights & biases using gradients, we can use the optimizer."""

# Define optimizer
opt = tr.optim.SGD(model.parameters(), lr=1e-5)

# Utility function to train the model
def fit(num_epochs, model, loss_fn, opt, train_dl):
    
    # Repeat for given number of epochs
    for epoch in range(num_epochs):
        
        # Train with batches of data
        for xb,yb in train_dl:
            
            # 1. Generate predictions
            pred = model(xb)
            
            # 2. Calculate loss
            loss = loss_fn(pred, yb)
            
            # 3. Compute gradients
            loss.backward()
            
            # 4. Update parameters using gradients
            opt.step()
            
            # 5. Reset the gradients to zero
            opt.zero_grad()
        
        # Print the progress
        if (epoch+1) % 10 == 0:
            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))

fit(120, model, loss_fn, opt, train_dl)

# Generate predictions
preds = model(inputs)
preds

# Compare with targets
targets

model(tr.tensor([[75, 63, 44.]]))
